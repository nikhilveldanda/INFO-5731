{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In-class-exercise-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilveldanda/INFO-5731/blob/main/In-Class-exercises/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIwP2GDT9329"
      },
      "source": [
        "## The third In-class-exercise (9/15/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXPzXW7a932-"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8PUSblu932-"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "MJOCjJd6932_",
        "outputId": "532c3d91-8709-413a-8bc8-a456acfd8509"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "\"\"\"\n",
        "Research question - What is the review of Apple airpods. Whether it has positive reviews or negative reviews.\n",
        "Data required for this research is reviews of the customer who already bought the airpods. Ecommerce website contains the reviews of the product. I have choosen the amazon to\n",
        "collect the reviews of the Apple airpods. We can analyze the reviews by considering the words in them that is whether the user customer has written in a positive way\n",
        "or negative way. We need minimum 1000 reviews to get nearly accurate results.\n",
        "\n",
        "Steps for Collecting and Saving Data:\n",
        "I have used the BeautifulSoup library to extract the information from the website.\n",
        "I have extracted the reviews by using the classname and then appended to the empty list.\n",
        "To extract 500 reviews I have itearted 50 times as each page contains 10 reviews and I have generated the url dynamically while iterating\n",
        "Then created a dataframe form the list and then converted dataframe to csv\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nResearch question - What is the review of Apple airpods. Whether it has positive reviews or negative reviews.\\nData required for this research is reviews of the customer who already bought the airpods. Ecommerce website contains the reviews of the product. I have choosen the amazon to\\ncollect the reviews of the Apple airpods. We can analyze the reviews by considering the words in them that is whether the user customer has written in a positive way\\nor negative way. We need minimum 1000 reviews to get nearly accurate results.\\n\\nSteps for Collecting and Saving Data:\\nI have used the BeautifulSoup library to extract the information from the website.\\nI have extracted the reviews by using the classname and then appended to the empty list.\\nTo extract 500 reviews I have itearted 50 times as each page contains 10 reviews and I have generated the url dynamically while iterating\\nThen created a dataframe form the list and then converted dataframe to csv\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeDY981Z932_"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlYNgukfxJ5w",
        "outputId": "67ff24ae-2fa2-449c-c1ff-d2ea941c304a"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 112 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 143 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 225 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 904 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [67.4 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,326 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,761 kB]\n",
            "Fetched 11.8 MB in 4s (3,277 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 91.8 MB of archives.\n",
            "After this operation, 315 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 92.0.4515.159-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 92.0.4515.159-0ubuntu0.18.04.1 [81.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 92.0.4515.159-0ubuntu0.18.04.1 [4,026 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 92.0.4515.159-0ubuntu0.18.04.1 [4,902 kB]\n",
            "Fetched 91.8 MB in 6s (16.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155013 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_92.0.4515.159-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Q_vseL6Ero"
      },
      "source": [
        "import pandas as pd \n",
        "# Import Selenium and its sub libraries\n",
        "import selenium \n",
        "from selenium import webdriver\n",
        "# Import BS4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pandas.core.frame import DataFrame"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCe0SvUPB_99"
      },
      "source": [
        "def get_reviews(url) -> DataFrame:\n",
        "  driver.get(url)\n",
        "  driver.implicitly_wait(1) \n",
        "  # Set up action to click on 'load more' button\n",
        "  page = 1\n",
        "  #We want at least 1000 review, so get 50 at a safe number\n",
        "  while page<50:  \n",
        "      try:\n",
        "          load_more = driver.find_element_by_id('load-more-trigger')\n",
        "          load_more.click()\n",
        "          page+=1 \n",
        "      except:\n",
        "          break\n",
        "  # After fully expand the page, we will grab data from whole website\n",
        "  review = driver.find_elements_by_class_name('review-container')\n",
        "\n",
        "  title = []\n",
        "  content = []\n",
        "  rating = []\n",
        "  date = []\n",
        "  user_name = []\n",
        "  for n in range(len(review)):\n",
        "      try:\n",
        "          #Some reviewers only give review text or rating without the other, \n",
        "          #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
        "          #Check if each review has all the elements\n",
        "          ftitle = review[n].find_element_by_class_name('title').text\n",
        "          fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
        "          frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
        "          fdate = review[n].find_element_by_class_name('review-date').text\n",
        "          fname = review[n].find_element_by_class_name('display-name-link').text\n",
        "          #Then add them to the respective list\n",
        "          title.append(ftitle)\n",
        "          content.append(fcontent)\n",
        "          rating.append(frating)\n",
        "          date.append(fdate)\n",
        "          user_name.append(fname)\n",
        "      except:\n",
        "          continue\n",
        "  #Build data dictionary for dataframe\n",
        "  data = {'User name': user_name, \n",
        "          'Star': rating,\n",
        "          'Review title': title,\n",
        "          'Review text' : content,\n",
        "          'Review posted time' : date,\n",
        "          \n",
        "      }\n",
        "  #Build dataframe for each movie to export\n",
        "  review = pd.DataFrame(data = data)\n",
        "  return review"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "YNtlXQ8mBWdn",
        "outputId": "6dec102d-c05b-4f54-8d42-0dac3eb9a797"
      },
      "source": [
        "get_reviews(\"https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User name</th>\n",
              "      <th>Star</th>\n",
              "      <th>Review title</th>\n",
              "      <th>Review text</th>\n",
              "      <th>Review posted time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mysticfall</td>\n",
              "      <td>10/10</td>\n",
              "      <td>For those who didn't like the movie because of...</td>\n",
              "      <td>It's not really a review but my attempt to exp...</td>\n",
              "      <td>18 October 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jeremy_Urquhart</td>\n",
              "      <td>10/10</td>\n",
              "      <td>One of the best films of this decade</td>\n",
              "      <td>I am remarkably stingy with my 10/10 ratings. ...</td>\n",
              "      <td>5 July 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jtindahouse</td>\n",
              "      <td>8/10</td>\n",
              "      <td>You name a genre, this movie covers it</td>\n",
              "      <td>I can't remember the last time I saw a movie t...</td>\n",
              "      <td>6 October 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nehpetstephen</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Meritocracy: it's metaphorical</td>\n",
              "      <td>In a meritocracy, success and fortune are rese...</td>\n",
              "      <td>25 August 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>keezo9uno</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A true masterpiece.</td>\n",
              "      <td>This movie is a gosh darn masterpiece. It will...</td>\n",
              "      <td>19 August 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>nikos_belitsis</td>\n",
              "      <td>3/10</td>\n",
              "      <td>Good Idea, but...</td>\n",
              "      <td>I have to admit that the main idea of this Kor...</td>\n",
              "      <td>8 September 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1201</th>\n",
              "      <td>madbird-61243</td>\n",
              "      <td>7/10</td>\n",
              "      <td>Not enjoyable to me</td>\n",
              "      <td>Cannot understand why it can win important awa...</td>\n",
              "      <td>29 August 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>thebeeteekay</td>\n",
              "      <td>2/10</td>\n",
              "      <td>sooo bad that it is good?</td>\n",
              "      <td>Parasite is a rare movie -\\nthe kind of movie ...</td>\n",
              "      <td>19 February 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>euphorion</td>\n",
              "      <td>3/10</td>\n",
              "      <td>From the cellar, the quality goes down</td>\n",
              "      <td>A huge disappointment. Had the film ended afte...</td>\n",
              "      <td>22 December 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>pglove</td>\n",
              "      <td>6/10</td>\n",
              "      <td>Bit of a Stinker Unfortunately</td>\n",
              "      <td>10/10 for acting, cinematography, and plot dev...</td>\n",
              "      <td>12 December 2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1205 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            User name  ... Review posted time\n",
              "0          mysticfall  ...    18 October 2019\n",
              "1     Jeremy_Urquhart  ...        5 July 2019\n",
              "2         jtindahouse  ...     6 October 2019\n",
              "3       nehpetstephen  ...     25 August 2019\n",
              "4           keezo9uno  ...     19 August 2019\n",
              "...               ...  ...                ...\n",
              "1200   nikos_belitsis  ...   8 September 2019\n",
              "1201    madbird-61243  ...     29 August 2019\n",
              "1202     thebeeteekay  ...   19 February 2020\n",
              "1203        euphorion  ...   22 December 2019\n",
              "1204           pglove  ...   12 December 2019\n",
              "\n",
              "[1205 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tphGifcv933A"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8bJVWXaJRY"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "import pandas as pd\n",
        "\n",
        "proxies = {\n",
        "  'http': os.getenv('HTTP_PROXY') # or just type proxy here without os.getenv()\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'User-agent':\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "for page in range(0,2000,10):\n",
        "  params = {\n",
        "    \"q\": \"nlp\",\n",
        "    \"hl\": \"en\",\n",
        "    \"start\" : page\n",
        "  }\n",
        "  html = requests.get('https://scholar.google.com/scholar', headers=headers, params=params, proxies=proxies).text\n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "\n",
        "  # JSON data will be collected here\n",
        "\n",
        "  # Container where all needed data is located\n",
        "  for result in soup.select('.gs_ri'):\n",
        "    try:\n",
        "      title = result.select_one('.gs_rt').text\n",
        "      publication_info = result.select_one('.gs_a').text\n",
        "      snippet = result.select_one('.gs_rs').text\n",
        "      Author = result.select_one('.gs_a').text.split(\",\")[0],\n",
        "      Year= result.select_one('.gs_a').text.split(\",\")[-1][1:5]\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    data.append({\n",
        "      'title': title,\n",
        "      'publication_info': publication_info,\n",
        "      \"Author\" : Author,\n",
        "      \"Year\" :Year,\n",
        "      'Abstract': snippet\n",
        "    })\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "0fhfK5bBu0Ij",
        "outputId": "92a2a110-97cd-4cf3-f560-ac6ea1ab3f7b"
      },
      "source": [
        "df = pd.DataFrame(data=data)\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>publication_info</th>\n",
              "      <th>Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jumping NLP curves: A review of natural langua...</td>\n",
              "      <td>E Cambria, B White - IEEE Computational intell...</td>\n",
              "      <td>(E Cambria,)</td>\n",
              "      <td>2014</td>\n",
              "      <td>Natural language processing (NLP) is a theory-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[PDF][PDF] BRAT: a web-based tool for NLP-assi...</td>\n",
              "      <td>P Stenetorp, S Pyysalo, G Topić, T Ohta… - Pro...</td>\n",
              "      <td>(P Stenetorp,)</td>\n",
              "      <td>2012</td>\n",
              "      <td>We introduce the brat rapid annotation tool (B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Energy and policy considerations for deep lear...</td>\n",
              "      <td>E Strubell, A Ganesh, A McCallum - arXiv prepr...</td>\n",
              "      <td>(E Strubell,)</td>\n",
              "      <td>2019</td>\n",
              "      <td>Recent progress in hardware and methodology fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BERT rediscovers the classical NLP pipeline</td>\n",
              "      <td>I Tenney, D Das, E Pavlick - arXiv preprint ar...</td>\n",
              "      <td>(I Tenney,)</td>\n",
              "      <td>2019</td>\n",
              "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PDF][PDF] Instance weighting for domain adapt...</td>\n",
              "      <td>J Jiang, CX Zhai - Proceedings of the 45th ann...</td>\n",
              "      <td>(J Jiang,)</td>\n",
              "      <td>2007</td>\n",
              "      <td>Abstract Domain adaptation is an important pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>Kişisel Gelişim Kitaplarının Eleştirel Bir Değ...</td>\n",
              "      <td>İ Özdemir - Ankara Üniversitesi Sosyal Bilimle...</td>\n",
              "      <td>(İ Özdemir - Ankara Üniversitesi Sosyal Biliml...</td>\n",
              "      <td>2010</td>\n",
              "      <td>Turkish Abstract: İletişim becerileri kavramın...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>[BOOK][B] Der Frosch auf der Butter-NLP-Die Gr...</td>\n",
              "      <td>H Krusche - 2019 - books.google.com</td>\n",
              "      <td>(H Krusche - 2019 - books.google.com,)</td>\n",
              "      <td>Kru</td>\n",
              "      <td>Ein Muss fuer alle Positivdenker! Mit NLP, der...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>[PDF][PDF] What is NLP?</td>\n",
              "      <td>R Dilts, R Bandler, J Grinder, J DeLozier - Ре...</td>\n",
              "      <td>(R Dilts,)</td>\n",
              "      <td>1999</td>\n",
              "      <td>Ce este programarea neurolingvistici? in timp ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867</th>\n",
              "      <td>[PDF][PDF] Szte-nlp: Sentiment detection on tw...</td>\n",
              "      <td>V Hangya, G Berend, R Farkas - 2013 - publicat...</td>\n",
              "      <td>(V Hangya,)</td>\n",
              "      <td>R Fa</td>\n",
              "      <td>In this paper we introduce our contribution to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>Biomedical term extraction: Nlp techniques in ...</td>\n",
              "      <td>AM Sandoval, J Díaz, LC Llanos, T Redondo - IJ...</td>\n",
              "      <td>(AM Sandoval,)</td>\n",
              "      <td>2019</td>\n",
              "      <td>Artificial Intelligence (AI) and its branch Na...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>869 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title  ...                                           Abstract\n",
              "0    Jumping NLP curves: A review of natural langua...  ...  Natural language processing (NLP) is a theory-...\n",
              "1    [PDF][PDF] BRAT: a web-based tool for NLP-assi...  ...  We introduce the brat rapid annotation tool (B...\n",
              "2    Energy and policy considerations for deep lear...  ...  Recent progress in hardware and methodology fo...\n",
              "3          BERT rediscovers the classical NLP pipeline  ...  Pre-trained text encoders have rapidly advance...\n",
              "4    [PDF][PDF] Instance weighting for domain adapt...  ...  Abstract Domain adaptation is an important pro...\n",
              "..                                                 ...  ...                                                ...\n",
              "864  Kişisel Gelişim Kitaplarının Eleştirel Bir Değ...  ...  Turkish Abstract: İletişim becerileri kavramın...\n",
              "865  [BOOK][B] Der Frosch auf der Butter-NLP-Die Gr...  ...  Ein Muss fuer alle Positivdenker! Mit NLP, der...\n",
              "866                            [PDF][PDF] What is NLP?  ...  Ce este programarea neurolingvistici? in timp ...\n",
              "867  [PDF][PDF] Szte-nlp: Sentiment detection on tw...  ...  In this paper we introduce our contribution to...\n",
              "868  Biomedical term extraction: Nlp techniques in ...  ...  Artificial Intelligence (AI) and its branch Na...\n",
              "\n",
              "[869 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EiYvDfr933B"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1qm31u-HJF9",
        "outputId": "a290fe46-0b78-482f-95e6-e7f5243ba94e"
      },
      "source": [
        "!pip install tweepy\n",
        "import tweepy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-7GoR2B933B"
      },
      "source": [
        "def get_tweets(user,count):\n",
        "  # Twitter api's secrets\n",
        "  consumer_key = \"D8SEFTp7FCUJJ5FQmfYngbCys\"\n",
        "  consumer_secret = \"eQJCdjNQjTCE4KbyrhQAOWM01wtUBSBigGeZpGVmolPZw12W98\"\n",
        "  access_token = \"1446873278-nRT9fD1H3rBtJP3Fd4VKQCFKFxsPOS335nVENTr\"\n",
        "  access_token_secret = \"ZBdOdGRcAsclJkaHOQJ8k006hgw0CuIeNZyj1bLhp8uKr\"\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "  username = user\n",
        "  count = count\n",
        "  try:     \n",
        "    # Creation of query method using parameters\n",
        "    tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
        "    \n",
        "    # Pulling information from tweets iterable object\n",
        "    tweets_list = [[tweet.user.screen_name, tweet.created_at, tweet.text] for tweet in tweets]\n",
        "    \n",
        "    # Creation of dataframe from tweets list\n",
        "    # Add or remove columns as you remove tweet information\n",
        "    tweets_df = pd.DataFrame(tweets_list)\n",
        "    tweets_df.rename(columns={0: \"User_name\", 1: \"Posted time\",2: \"Text\"},inplace=True)\n",
        "  except BaseException as e:\n",
        "    print('failed on_status,',str(e))\n",
        "    time.sleep(3)\n",
        "  \n",
        "  return tweets_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "sdZ3VJ8SHCyV",
        "outputId": "d5bfe321-d6ed-47d9-8bb5-68c4b957c4e5"
      },
      "source": [
        "get_tweets(\"POTUS\",1000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_name</th>\n",
              "      <th>Posted time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-09-19 18:00:01</td>\n",
              "      <td>We need to build an economy that gives working...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-09-19 14:46:00</td>\n",
              "      <td>For me it’s pretty simple: It’s about time wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-09-18 18:05:14</td>\n",
              "      <td>My plan is very clear: we will not raise taxes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-09-18 16:00:56</td>\n",
              "      <td>Big corporations and the super wealthy have to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-09-17 23:39:50</td>\n",
              "      <td>RT @WhiteHouse: Press Sec. Jen Psaki sat down ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-04-15 12:41:52</td>\n",
              "      <td>When we’ve invested in innovation throughout o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-04-15 00:20:01</td>\n",
              "      <td>I am immeasurably grateful for the bravery tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-04-14 20:31:54</td>\n",
              "      <td>Our diplomacy does not hinge on having U.S. bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-04-14 19:24:48</td>\n",
              "      <td>I am now the fourth American president to pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>POTUS</td>\n",
              "      <td>2021-04-14 19:05:01</td>\n",
              "      <td>We went to Afghanistan because of the horrific...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    User_name  ...                                               Text\n",
              "0       POTUS  ...  We need to build an economy that gives working...\n",
              "1       POTUS  ...  For me it’s pretty simple: It’s about time wor...\n",
              "2       POTUS  ...  My plan is very clear: we will not raise taxes...\n",
              "3       POTUS  ...  Big corporations and the super wealthy have to...\n",
              "4       POTUS  ...  RT @WhiteHouse: Press Sec. Jen Psaki sat down ...\n",
              "..        ...  ...                                                ...\n",
              "995     POTUS  ...  When we’ve invested in innovation throughout o...\n",
              "996     POTUS  ...  I am immeasurably grateful for the bravery tha...\n",
              "997     POTUS  ...  Our diplomacy does not hinge on having U.S. bo...\n",
              "998     POTUS  ...  I am now the fourth American president to pres...\n",
              "999     POTUS  ...  We went to Afghanistan because of the horrific...\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}